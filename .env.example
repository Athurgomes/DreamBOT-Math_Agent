#Se estiver usando Docker no Windows ou Mac, use: http://host.docker.internal:11434
#Se estiver rodando apenas localmente sem Docker, use: http://localhost:11434
OLLAMA_HOST=http://localhost:11434

#Nome exato do modelo baixado no Ollama verifique com 'ollama list'
OLLAMA_MODEL=llama3.1:8b

#Configuração do Frontend
# Se rodar via Docker Compose, use: http://backend:8000
# Se rodar localmente, use: http://localhost:8000
API_URL=http://localhost:8000